{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a14fab",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-05T04:51:19.815562Z",
     "iopub.status.busy": "2022-03-05T04:51:19.813913Z",
     "iopub.status.idle": "2022-03-05T04:51:22.770279Z",
     "shell.execute_reply": "2022-03-05T04:51:22.769262Z",
     "shell.execute_reply.started": "2022-03-05T04:48:27.572775Z"
    },
    "papermill": {
     "duration": 2.976567,
     "end_time": "2022-03-05T04:51:22.770436",
     "exception": false,
     "start_time": "2022-03-05T04:51:19.793869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('max_columns', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab54e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T04:51:22.859189Z",
     "iopub.status.busy": "2022-03-05T04:51:22.858377Z",
     "iopub.status.idle": "2022-03-05T04:51:22.860595Z",
     "shell.execute_reply": "2022-03-05T04:51:22.861000Z",
     "shell.execute_reply.started": "2022-03-05T04:48:30.156612Z"
    },
    "papermill": {
     "duration": 0.073605,
     "end_time": "2022-03-05T04:51:22.861167",
     "exception": false,
     "start_time": "2022-03-05T04:51:22.787562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim',\n",
    "                 'B-Counterclaim', 'I-Counterclaim', 'B-Rebuttal', 'I-Rebuttal',\n",
    "                 'B-Evidence', 'I-Evidence', 'B-Concluding', 'I-Concluding']\n",
    "\n",
    "replace_labels = {'O': 'O', 'B-Lead': 'Lead', 'I-Lead': 'Lead', 'B-Position': 'Position', 'I-Position': 'Position', \n",
    "                  'B-Claim': 'Claim', 'I-Claim': 'Claim', 'B-Counterclaim': 'Counterclaim', 'I-Counterclaim': 'Counterclaim', \n",
    "                  'B-Rebuttal': 'Rebuttal', 'I-Rebuttal': 'Rebuttal', 'B-Evidence': 'Evidence', 'I-Evidence': 'Evidence', \n",
    "                  'B-Concluding': 'Concluding Statement', 'I-Concluding': 'Concluding Statement'}\n",
    "\n",
    "num_labels = len(output_labels)\n",
    "key2val = {k: v for v, k in enumerate(output_labels)}\n",
    "val2key = {v: k for v, k in enumerate(output_labels)}\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f7fe29f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T04:51:22.916837Z",
     "iopub.status.busy": "2022-03-05T04:51:22.915897Z",
     "iopub.status.idle": "2022-03-05T04:51:22.917878Z",
     "shell.execute_reply": "2022-03-05T04:51:22.918320Z",
     "shell.execute_reply.started": "2022-03-05T04:48:30.204708Z"
    },
    "papermill": {
     "duration": 0.043129,
     "end_time": "2022-03-05T04:51:22.918459",
     "exception": false,
     "start_time": "2022-03-05T04:51:22.875330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    df = pd.DataFrame()\n",
    "    id_list, text_list = [], []\n",
    "    for filename in os.listdir(path):\n",
    "        id_list.append(filename.split('.')[0])\n",
    "        with open(f'{path}/{filename}') as file:\n",
    "            text_list.append(file.read())\n",
    "    df['id'] = id_list\n",
    "    df['text'] = text_list\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_entities(df, df_labels):\n",
    "    df['entities'] = ''\n",
    "    for i in range(len(df)):\n",
    "        current_num_words = len(df.loc[i, 'text'].split())\n",
    "        file_id = df.loc[i, 'id']\n",
    "        pos_character_start_list = df_labels.loc[df_labels['id'] == file_id, 'discourse_start'].astype('int').tolist()\n",
    "        pos_character_end_list = df_labels.loc[df_labels['id'] == file_id, 'discourse_end'].astype('int').tolist()\n",
    "        labels_list = df_labels.loc[df_labels['id'] == file_id, 'discourse_type'].tolist()\n",
    "        entities = ['O' for _ in range(len(df.loc[i, 'text'].split()))]\n",
    "        \n",
    "        for j in range(len(labels_list)):\n",
    "            pos_character_start, pos_character_end = pos_character_start_list[j], pos_character_end_list[j]\n",
    "            pos_word_start = len(df.loc[i, 'text'][:pos_character_start].split())\n",
    "            pos_word_end = len(df.loc[i, 'text'][:pos_character_end].split()) - 1\n",
    "            for k in range(pos_word_start, pos_word_end):\n",
    "                if k == pos_word_start:\n",
    "                    entities[k] = f'B-{labels_list[j].split()[0]}'\n",
    "                else:\n",
    "                    entities[k] = f'I-{labels_list[j].split()[0]}'\n",
    "        df.loc[i, 'entities'] = ' '.join(entities)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def text_f1(labels, preds, word_ids):\n",
    "    arr_length = 0\n",
    "    active_preds = dict()\n",
    "    active_labels = dict()\n",
    "    \n",
    "    for i in range(len(preds)):\n",
    "        active_preds[i] = []\n",
    "        active_labels[i] = []\n",
    "        last_word_id = -100\n",
    "        for j in range(len(preds[i])):\n",
    "            #print(f'label: {labels[i][j]} pred: {preds[i][j]}')\n",
    "            current_word_id = word_ids[i][j]\n",
    "            if labels[i][j] != -100 and current_word_id != last_word_id:\n",
    "                active_preds[i].append(preds[i][j])\n",
    "                active_labels[i].append(labels[i][j])\n",
    "                arr_length += 1\n",
    "            last_word_id = current_word_id\n",
    "\n",
    "    metric_preds = np.zeros(arr_length)\n",
    "    metric_labels = np.zeros(arr_length)\n",
    "    current_pos = 0\n",
    "    for i in range(len(active_preds)):\n",
    "        for j in range(len(active_preds[i])):\n",
    "            metric_preds[current_pos] = active_preds[i][j]\n",
    "            metric_labels[current_pos] = active_labels[i][j]\n",
    "            current_pos += 1\n",
    "    \n",
    "    return f1_score(metric_labels, metric_preds, average='weighted')\n",
    "\n",
    "\n",
    "def preds2submission(preds, file_id_list, word_ids):\n",
    "    id_list, label_list, predictionstring_list = [], [], []\n",
    "    for i in range(len(preds)):\n",
    "        last_label = 'O'\n",
    "        words_count = 0\n",
    "        predictionstring = ''\n",
    "        last_word_id = -100\n",
    "        is_not_finished = True\n",
    "        for j in range(len(preds[i])):\n",
    "            current_label = replace_labels[val2key[preds[i][j]]]\n",
    "            current_word_id = word_ids[i][j]\n",
    "            #print(f'cur_word_id: {current_word_id} last_word_id: {last_word_id} cur_label: {current_label} last_label: {last_label} i: {i} j: {j}')\n",
    "            if current_word_id == -100 or current_word_id == last_word_id:\n",
    "                #print('Continue')\n",
    "                \n",
    "                if current_word_id == -100 and j > 0 and last_label != 'O' and is_not_finished:\n",
    "                    id_list.append(file_id_list[i])\n",
    "                    label_list.append(last_label)\n",
    "                    predictionstring_list.append(predictionstring)\n",
    "                    is_not_finished = False\n",
    "                \n",
    "                continue\n",
    "            if current_label != last_label:\n",
    "                if last_label != 'O':\n",
    "                    id_list.append(file_id_list[i])\n",
    "                    label_list.append(last_label)\n",
    "                    predictionstring_list.append(predictionstring)\n",
    "                    \n",
    "                    if current_label != 'O':\n",
    "                        predictionstring = str(words_count)\n",
    "                else:\n",
    "                    predictionstring = str(words_count)\n",
    "            elif current_label == last_label and current_label != 'O':\n",
    "                predictionstring += f' {words_count}'\n",
    "            \n",
    "            \"\"\"if j == len(preds[i]) - 1 and current_label != 'O':\n",
    "                id_list.append(file_id_list[i])\n",
    "                label_list.append(current_label)\n",
    "                predictionstring_list.append(predictionstring)\"\"\"\n",
    "                \n",
    "            last_word_id = current_word_id\n",
    "            last_label = current_label\n",
    "            words_count += 1\n",
    "\n",
    "    print(f'id: {len(id_list)}')\n",
    "    print(f'class: {len(label_list)}')\n",
    "    print(f'pred: {len(predictionstring_list)}')\n",
    "    df = pd.DataFrame({'id': id_list, 'class': label_list, 'predictionstring': predictionstring_list})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5622b8b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T04:51:22.970370Z",
     "iopub.status.busy": "2022-03-05T04:51:22.949708Z",
     "iopub.status.idle": "2022-03-05T04:51:22.986931Z",
     "shell.execute_reply": "2022-03-05T04:51:22.987549Z",
     "shell.execute_reply.started": "2022-03-05T04:48:30.235036Z"
    },
    "papermill": {
     "duration": 0.055063,
     "end_time": "2022-03-05T04:51:22.987718",
     "exception": false,
     "start_time": "2022-03-05T04:51:22.932655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length, is_label=False):\n",
    "        self.text = data['text']\n",
    "        self.entities = data['entities'] if is_label else None\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_label = is_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoding = self.tokenizer(self.text[index].split(), is_split_into_words=True, truncation=True,\n",
    "                                  padding='max_length', max_length=self.max_length)\n",
    "        word_ids = encoding.word_ids()\n",
    "        return_word_ids = []\n",
    "\n",
    "        if self.is_label:\n",
    "            entities_list = self.entities[index].split()\n",
    "        labels = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                return_word_ids.append(-100)\n",
    "                if self.is_label:\n",
    "                    labels.append(-100)\n",
    "            else:\n",
    "                return_word_ids.append(word_id)\n",
    "                if self.is_label:\n",
    "                    labels.append(key2val[entities_list[word_id]])\n",
    "        \n",
    "        if self.is_label:\n",
    "            encoding['labels'] = labels\n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "\n",
    "        return item, np.array(return_word_ids)\n",
    "\n",
    "\n",
    "class TextModel:\n",
    "    def __init__(self, model_name, max_length):\n",
    "        self.model_name = model_name\n",
    "        self.num_folds = 3\n",
    "        self.tokenizer = None\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def fit(self, data, batch_size=4, lr=[2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7], epochs=5, num_folds=1):\n",
    "        \n",
    "        self.num_folds = num_folds\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, add_prefix_space=True)\n",
    "\n",
    "        full_preds = np.zeros((len(data), self.max_length))\n",
    "        full_labels = np.zeros((len(data), self.max_length))\n",
    "\n",
    "        kfold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "        current_fold = 0\n",
    "        for fold, (trn_ind, val_ind) in enumerate(kfold.split(data)):\n",
    "            \n",
    "            if fold not in [0]:\n",
    "                continue\n",
    "            \n",
    "            data_train = data.loc[trn_ind]\n",
    "            data_train.index = range(len(data_train))\n",
    "            data_val = data.loc[val_ind]\n",
    "            data_val.index = range(len(data_val))\n",
    "\n",
    "            dataset_train = TextDataset(data_train, self.tokenizer, max_length=self.max_length, is_label=True)\n",
    "            loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "            dataset_val = TextDataset(data_val, self.tokenizer, max_length=self.max_length, is_label=True)\n",
    "            loader_val = DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "            model = AutoModelForTokenClassification.from_pretrained(self.model_name, num_labels=num_labels).to(device)\n",
    "            \n",
    "            optimizer = optim.Adam(params=model.parameters(), lr=lr[0])\n",
    "            #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                print(f'epoch: {epoch+1}')\n",
    "                model.train()\n",
    "                for batch, _ in tqdm(loader_train):\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    \n",
    "                    for g in optimizer.param_groups:\n",
    "                        g['lr'] = lr[epoch]\n",
    "\n",
    "                    loss, _ = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, return_dict=False)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    current_pos = 0\n",
    "                    temp_preds = np.zeros((len(data_val), self.max_length))\n",
    "                    temp_labels = np.zeros((len(data_val), self.max_length))\n",
    "                    temp_words = np.zeros((len(data), self.max_length))\n",
    "                    for batch, word_ids in tqdm(loader_val):\n",
    "                        input_ids = batch['input_ids'].to(device)\n",
    "                        attention_mask = batch['attention_mask'].to(device)\n",
    "                        labels = batch['labels'].to(device)\n",
    "                        current_batch_size = len(batch['input_ids'])\n",
    "\n",
    "                        _, logit_preds = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                               labels=labels, return_dict=False)\n",
    "                        flattened_preds = torch.argmax(logit_preds.view(-1, model.num_labels), dim=1)\n",
    "                        temp_preds[current_pos:current_pos+current_batch_size] = flattened_preds.view(current_batch_size, -1).cpu().numpy()\n",
    "                        temp_labels[current_pos:current_pos+current_batch_size] = labels.cpu().numpy()\n",
    "                        temp_words[current_pos:current_pos+current_batch_size] = word_ids\n",
    "                        current_pos += current_batch_size\n",
    "                    full_preds[val_ind] = temp_preds\n",
    "                    full_labels[val_ind] = temp_labels\n",
    "                    \n",
    "                    print(f'f1_score: {text_f1(temp_labels, temp_preds, temp_words)} fold: {current_fold}')\n",
    "                \n",
    "                #scheduler.step()\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.save(model.state_dict(), f'./{self.model_name.split(\"/\")[-1]}{current_fold+1}.pt')\n",
    "            current_fold += 1\n",
    "\n",
    "        print(f'full_preds: {full_preds} shape: {full_preds.shape}')\n",
    "\n",
    "        return full_preds, full_labels\n",
    "    \n",
    "    def predict(self, data, batch_size=4, is_label=False):\n",
    "        dataset = TextDataset(data, self.tokenizer, max_length=self.max_length, is_label=is_label)\n",
    "        loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "        \n",
    "        preds = np.zeros((len(data), self.max_length, num_labels))\n",
    "        labels = np.zeros((len(data), self.max_length))\n",
    "        words = np.zeros((len(data), self.max_length))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for fold in range(self.num_folds):\n",
    "                model = AutoModelForTokenClassification.from_pretrained(self.model_name, num_labels=num_labels).to(device)\n",
    "                model.load_state_dict(torch.load(f'./{self.model_name.split(\"/\")[-1]}{fold+1}.pt'))\n",
    "                model.eval()\n",
    "                current_pos = 0\n",
    "                for batch, word_ids in tqdm(loader):\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels_temp = None\n",
    "                    current_batch_size = len(batch['input_ids'])\n",
    "                    if is_label:\n",
    "                        labels_temp = batch['labels'].to(device)\n",
    "                        _, logit_preds = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels_temp, return_dict=False)\n",
    "                        labels[current_pos:current_pos+batch_size] = labels_temp.cpu().numpy()\n",
    "                    else:\n",
    "                        logit_preds = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)[0]\n",
    "                    #print(f'\\nwids: {word_ids} \\nwids_len: {len(word_ids)} \\nwords: {words} \\nwords_len: {len(words)}')\n",
    "                    #print(f'w_ids_shape: {word_ids.shape} words_shape: {words.shape} words_batch_shape: {words[current_pos:current_pos+current_batch_size].shape}')\n",
    "                    preds[current_pos:current_pos+current_batch_size] += logit_preds.cpu().numpy()\n",
    "                    words[current_pos:current_pos+current_batch_size] = word_ids\n",
    "                    current_pos += current_batch_size\n",
    "        preds = preds / self.num_folds\n",
    "        return np.argmax(preds, axis=2), labels, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d55f8cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T04:51:23.025225Z",
     "iopub.status.busy": "2022-03-05T04:51:23.024313Z",
     "iopub.status.idle": "2022-03-05T09:39:41.699736Z",
     "shell.execute_reply": "2022-03-05T09:39:41.693968Z",
     "shell.execute_reply.started": "2022-03-05T04:48:30.273679Z"
    },
    "papermill": {
     "duration": 17298.696542,
     "end_time": "2022-03-05T09:39:41.699864",
     "exception": false,
     "start_time": "2022-03-05T04:51:23.003322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "Some weights of the model checkpoint at ../input/huggingfacebigbirdrobertabase were not used when initializing BigBirdForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at ../input/huggingfacebigbirdrobertabase and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3509 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\n",
      "  return torch.floor_divide(self, other)\n",
      "100%|██████████| 3509/3509 [52:13<00:00,  1.12it/s]\n",
      "100%|██████████| 390/390 [01:44<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.7625139235728716 fold: 0\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3509/3509 [52:11<00:00,  1.12it/s]\n",
      "100%|██████████| 390/390 [01:44<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.7697419510132478 fold: 0\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3509/3509 [52:15<00:00,  1.12it/s]\n",
      "100%|██████████| 390/390 [01:44<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.7805343580703379 fold: 0\n",
      "epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3509/3509 [52:19<00:00,  1.12it/s]\n",
      "100%|██████████| 390/390 [01:44<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.7805882612349312 fold: 0\n",
      "epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3509/3509 [52:20<00:00,  1.12it/s]\n",
      "100%|██████████| 390/390 [01:44<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.7812427660791504 fold: 0\n",
      "full_preds: [[ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 2.  1.  2. ... 12. 12. 12.]] shape: (15594, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingfacebigbirdrobertabase were not used when initializing BigBirdForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at ../input/huggingfacebigbirdrobertabase and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.24it/s]\n"
     ]
    }
   ],
   "source": [
    "df_labels = pd.read_csv('../input/feedback-prize-2021/train.csv')\n",
    "df_train = read_data('../input/feedback-prize-2021/train')\n",
    "\n",
    "#df_train, df_val = train_test_split(df_train, test_size=0.2, shuffle=True, random_state=0)\n",
    "\n",
    "df_train.index = range(len(df_train))\n",
    "df_train = create_entities(df_train, df_labels)\n",
    "\n",
    "#df_val.index = range(len(df_val))\n",
    "#df_val = create_entities(df_val, df_labels)\n",
    "\n",
    "df_test = read_data('../input/feedback-prize-2021/test')\n",
    "\n",
    "model = TextModel(model_name='../input/huggingfacebigbirdrobertabase', max_length=1024)\n",
    "#model = TextModel(model_name='../input/py-bigbird-v26', max_length=1024)\n",
    "val_preds, val_labels = model.fit(df_train)\n",
    "#preds, labels, word_ids = model.predict(df_val, is_label=True)\n",
    "test_preds, _, test_word_ids = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f96da736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T09:39:54.491385Z",
     "iopub.status.busy": "2022-03-05T09:39:54.490250Z",
     "iopub.status.idle": "2022-03-05T09:39:54.492777Z",
     "shell.execute_reply": "2022-03-05T09:39:54.492285Z",
     "shell.execute_reply.started": "2022-03-05T04:50:02.846736Z"
    },
    "papermill": {
     "duration": 6.13577,
     "end_time": "2022-03-05T09:39:54.492899",
     "exception": false,
     "start_time": "2022-03-05T09:39:48.357129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#text_f1(labels, preds, word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4cedacd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T09:40:07.290941Z",
     "iopub.status.busy": "2022-03-05T09:40:07.279220Z",
     "iopub.status.idle": "2022-03-05T09:40:07.305609Z",
     "shell.execute_reply": "2022-03-05T09:40:07.305123Z",
     "shell.execute_reply.started": "2022-03-05T04:50:02.866350Z"
    },
    "papermill": {
     "duration": 6.410882,
     "end_time": "2022-03-05T09:40:07.305725",
     "exception": false,
     "start_time": "2022-03-05T09:40:00.894843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 75\n",
      "class: 75\n",
      "pred: 75\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Lead</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Position</td>\n",
       "      <td>41 42 43 44 45 46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Claim</td>\n",
       "      <td>49 50 51 52 53 54 55 56 57 58 59 60 61 62 63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Claim</td>\n",
       "      <td>66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Claim</td>\n",
       "      <td>84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>Claim</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>150 151 152 153 154 155 156 157 158 159 160 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>223 224 225 226 227 228 229 230 231 232 233 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>306 307 308 309 310 311 312 313 314 315 316 31...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                 class  \\\n",
       "0   0FB0700DAF44                  Lead   \n",
       "1   0FB0700DAF44              Position   \n",
       "2   0FB0700DAF44                 Claim   \n",
       "3   0FB0700DAF44                 Claim   \n",
       "4   0FB0700DAF44                 Claim   \n",
       "..           ...                   ...   \n",
       "70  D46BCB48440A                 Claim   \n",
       "71  D46BCB48440A              Evidence   \n",
       "72  D46BCB48440A              Evidence   \n",
       "73  D46BCB48440A              Evidence   \n",
       "74  D46BCB48440A  Concluding Statement   \n",
       "\n",
       "                                     predictionstring  \n",
       "0   0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
       "1                                   41 42 43 44 45 46  \n",
       "2        49 50 51 52 53 54 55 56 57 58 59 60 61 62 63  \n",
       "3   66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 8...  \n",
       "4   84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 9...  \n",
       "..                                                ...  \n",
       "70                                                 44  \n",
       "71  56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 7...  \n",
       "72  150 151 152 153 154 155 156 157 158 159 160 16...  \n",
       "73  223 224 225 226 227 228 229 230 231 232 233 23...  \n",
       "74  306 307 308 309 310 311 312 313 314 315 316 31...  \n",
       "\n",
       "[75 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = preds2submission(test_preds, df_test['id'], test_word_ids)\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46273d35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T09:40:20.466962Z",
     "iopub.status.busy": "2022-03-05T09:40:20.466169Z",
     "iopub.status.idle": "2022-03-05T09:40:20.484548Z",
     "shell.execute_reply": "2022-03-05T09:40:20.484913Z",
     "shell.execute_reply.started": "2022-03-05T04:50:02.902719Z"
    },
    "papermill": {
     "duration": 6.518239,
     "end_time": "2022-03-05T09:40:20.485051",
     "exception": false,
     "start_time": "2022-03-05T09:40:13.966812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b997827",
   "metadata": {
    "papermill": {
     "duration": 6.351145,
     "end_time": "2022-03-05T09:40:32.955082",
     "exception": false,
     "start_time": "2022-03-05T09:40:26.603937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced83d57",
   "metadata": {
    "papermill": {
     "duration": 6.834019,
     "end_time": "2022-03-05T09:40:45.935224",
     "exception": false,
     "start_time": "2022-03-05T09:40:39.101205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c174b8ff",
   "metadata": {
    "papermill": {
     "duration": 6.072994,
     "end_time": "2022-03-05T09:40:58.431661",
     "exception": false,
     "start_time": "2022-03-05T09:40:52.358667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1209e04",
   "metadata": {
    "papermill": {
     "duration": 6.179371,
     "end_time": "2022-03-05T09:41:11.043418",
     "exception": false,
     "start_time": "2022-03-05T09:41:04.864047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5056591",
   "metadata": {
    "papermill": {
     "duration": 6.373147,
     "end_time": "2022-03-05T09:41:24.247928",
     "exception": false,
     "start_time": "2022-03-05T09:41:17.874781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee3c1e",
   "metadata": {
    "papermill": {
     "duration": 6.427954,
     "end_time": "2022-03-05T09:41:36.855339",
     "exception": false,
     "start_time": "2022-03-05T09:41:30.427385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d923629",
   "metadata": {
    "papermill": {
     "duration": 6.838537,
     "end_time": "2022-03-05T09:41:49.932836",
     "exception": false,
     "start_time": "2022-03-05T09:41:43.094299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff963a88",
   "metadata": {
    "papermill": {
     "duration": 6.452258,
     "end_time": "2022-03-05T09:42:02.544923",
     "exception": false,
     "start_time": "2022-03-05T09:41:56.092665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b26bfc",
   "metadata": {
    "papermill": {
     "duration": 6.134839,
     "end_time": "2022-03-05T09:42:15.071284",
     "exception": false,
     "start_time": "2022-03-05T09:42:08.936445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17473.632528,
   "end_time": "2022-03-05T09:42:25.026971",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-05T04:51:11.394443",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
